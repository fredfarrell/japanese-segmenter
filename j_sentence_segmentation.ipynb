{
 "metadata": {
  "name": "",
  "signature": "sha256:973151cae0e1cc4fb0890551ca78eb3284b68b4b594e486f47109ac0256f021c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from collections import defaultdict\n",
      "import re\n",
      "import nltk\n",
      "import unicodedata\n",
      "import pickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook I use a sentence-tagging model called a Global Linear Model to perform segmentation of Japanese sentences into words. The model was trained on data from the KNB corpus of tagged Japanese sentences available via the python Natural Language Toolkit. To turn the segmentation problem into a tagging problem, each character is tagged as 'S' (only character in a word), 'B' (beginning of a word), 'M' (within a word), or 'E' (end of a word). See j_corpus.ipynb in this repository."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Functions needed to find a tag sequence (segmentation) given the GLM parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tags = ['S', 'B','M','E', '\u00a7']\n",
      "\n",
      "def dotprod(k,g):\n",
      "    return sum([k[x]*g[x] for x in g.keys()])\n",
      "\n",
      "#viterbi algorithm to 'decode' (find the best tag sequence given parameters)\n",
      "def viterbi(k,g,sentence):\n",
      "    \n",
      "    pi = {} #max prob\n",
      "    bp = {} #backpointer\n",
      "    tag_sequence = {} #tag\n",
      "    pi[(0,'*','*')] = 0\n",
      "    \n",
      "    #set up tag sets K_i\n",
      "    n = len(sentence)\n",
      "    K = {i:tags if i>0 else ['*'] for i in range(-1,n+1)}\n",
      "    \n",
      "    for j in range(1,n):\n",
      "        \n",
      "        for u in K[j-1]:\n",
      "            for v in K[j]:\n",
      "                \n",
      "                probs = [pi[(j-1,t,u)] + dotprod(k, g(t,u,sentence,j,v)) for t in K[j-2]]\n",
      "                pi[(j,u,v)] = np.max(probs)\n",
      "                bp[(j,u,v)] = K[j-2][np.argmax(probs)]\n",
      "    \n",
      "    try:\n",
      "        final = [pi[(n-1,u,s)] + dotprod(k, g(u,s,sentence,n,'\u00a7') ) for u in K[n-2] for s in K[n-1]]\n",
      "        final = np.array(final).reshape(len(K[n-2]),len(K[n-1]))\n",
      "    except:\n",
      "        print sentence\n",
      "        print n\n",
      "        raise\n",
      "    \n",
      "    tag_sequence[n-2] = K[n-2][np.unravel_index(final.argmax(),final.shape)[0]]\n",
      "    tag_sequence[n-1] = K[n-1][np.unravel_index(final.argmax(),final.shape)[1]]\n",
      "    \n",
      "    #reconstruct\n",
      "    for j in range(n-3,-1,-1):\n",
      "        tag_sequence[j] = bp[(j+2,tag_sequence[j+1],tag_sequence[j+2])]\n",
      "        \n",
      "    return [tag_sequence[i] for i in range(n)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. Function to calculate the 'features' of a sentence and tag sequence. The GLM uses these features to give a measure of how well the tag sequence matches the sentence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "'Features' are things like pairs of characters and tags, e.g. the pair ('\u3092','S') will have a positive weight as the character '\u3092' is almost always an entire word. Other features include the 'type' (Kanji/kana/numeral etc.) of the current character and those around it, since for example switching from katakana to hiragana usually indicates a word boundary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get character types for use in features\n",
      "def char_type(ch):\n",
      "    \n",
      "        ch = unicodedata.name(ch)\n",
      "        \n",
      "        if ch == u'\u00a7':\n",
      "            return '__STOP__'\n",
      "        if re.search('FULL STOP|COMMA',ch):\n",
      "            return '__PUNCT__'\n",
      "        if re.search('IDEOGRAPH',ch):\n",
      "            return '__KANJI__'\n",
      "        elif re.search('HIRAGANA', ch):\n",
      "            return '__HIRA__'\n",
      "        elif re.search('KATAKANA', ch):\n",
      "            return '__KATA__'\n",
      "        elif re.search('DIGIT', ch):\n",
      "            return '__NUM__'\n",
      "        elif re.search('LETTER', ch):\n",
      "            return '__ROMA__'\n",
      "        else:\n",
      "            return '__UNKNOWN__'\n",
      "        \n",
      "#function g to extract features from sentences\n",
      "def g_jseg(tag1,tag2,x,w,this_tag):\n",
      "    \n",
      "    x += u'\u00a7' #use this as stop symbol\n",
      "    \n",
      "    #trigrams and character-tag\n",
      "    d = defaultdict(int)\n",
      "    #d[(tag1,tag2,this_tag)] = 1\n",
      "    d[(x[w],this_tag)] = 1\n",
      "        \n",
      "    #this and next character\n",
      "    if w < len(x)-1:\n",
      "        d[('_NEXT_',x[w],x[w+1],this_tag)] = 1\n",
      "        \n",
      "    #type of current char, next char, last char (Kanji/Kana/Num/etc.)\n",
      "    if w>0 and w<(len(x)-1) :\n",
      "        d[('_TYPES_', char_type(x[w]), char_type(x[w-1]), char_type(x[w+1]), this_tag)] = 1\n",
      "        d[('_TYPES_CHAR_', x[w], char_type(x[w-1]), char_type(x[w+1]), this_tag)] = 1\n",
      "\n",
      "    \n",
      "    return d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3. Load in the parameter vector of the previously trained GLM"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v = pickle.load(open('j_seg_weights.pickle', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4. Finally, we need to convert tag sequences to a sentence with spaces between words "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def conv_tag_output(sentence,tags):\n",
      "    s = ''\n",
      "    \n",
      "    for c,t in zip(sentence,tags):\n",
      "        if t in ['S','B']:\n",
      "            s+=' '+c\n",
      "        else:\n",
      "            s+=c\n",
      "            \n",
      "    return s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can enter a Japanese sentence, and the algorithm will find the 'best' segmentation into words. If you know any Japaanese, try changing the sentence to explore how well the algorithm works! As you can see, it's by no means perfect but broadly correct (its accuracy at tagging boundaries correctly is about 85%). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = u'\u30d3\u30a4\u30eb\u3092\uff13\u676f\u98f2\u3093\u3060\u3002'\n",
      "print conv_tag_output(sentence,viterbi(v,g_jseg,sentence))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u30d3\u30a4\u30eb \u3092 \uff13 \u676f \u98f2\u3093\u3060 \u3002\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}